---
title: "HW2 STA521 Fall18"
author: '[Mingjie Zhao, mz136,mingjiezhao]'
date: "Due September 23, 2018 5pm"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Backgound Reading

Readings: Chapters 3-4 in Weisberg Applied Linear Regression


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(xtable)
library(tidyverse)
library(GGally)
suppressWarnings(library(alr3))
```

This exercise involves the UN data set from `alr3` package. Install `alr3` and the `car` packages and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chunks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed. Figures should have informative captions. Please switch the output to pdf for your final version to upload to Sakai. **Remove these instructions for final submission**


## Exploratory Data Analysis

0.  Preliminary read in the data.  After testing, modify the code chunk so that output, messages and warnings are suppressed.  *Exclude text from final*

```{r data }
#https://rweb.stat.umn.edu/R/site-library/alr3/html/UN3.html
data(UN3, package="alr3")
library(car)
```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?
\newline Answer: 6 variables have missing data, I think all the variables are quantitative variables
```{r}
#check first a few rows and summary
head(UN3)
summary(UN3)
#check characteristics of the variables
str(UN3)

```
\vspace{5mm}
2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r}
table1=t(as.matrix(rbind(round(apply(UN3[ 1:7],2,mean,na.rm=TRUE),3),round(apply(UN3[ 1:7],2,sd,na.rm=TRUE),3))))
colnames(table1)=c("mean","standard deviation")
kable(table1,caption = c("mean and standard deviation of the quantitative variables"))
```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?
\vspace{4mm}
\newline  Answer: from the ggpairs, we can see that the correlation values between ModernC and other variables range from 0.1 to -0.716. The PPdgp and Pop may need transformations because their distributions show bad normality. From plot generated by “pairs”, it is hard to tell the relationships between Pop/ PPdgp and other variables, indicating that pop and PPdgp need to be transformed to reveal its relationship with other variables. There are potential outliers in the plot of ModernC with Change, PPdgp and Pop.
```{r}
UN3_1=na.omit(UN3)
sp=ggpairs(UN3_1,  progress=F,title="Comparison Between Variables")
suppressWarnings(print(sp, progress=F))
pairs(UN3_1)

```

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?
\vspace{4mm}
\newline Answer: there are 125 observations are used in my model fitting. 
\newline From the residual plots: 
\newline 1) residuals vs fitted: the plot has relatively good constant variance, but there are some extreme data points, could be outliers
\newline 2) Normal Q-Q: the plot has heavy tails close to 3 std dev. The observed quartiles are larger than expected under normality assumption. There might be some outliers at both left and right sides
\newline 3) scale-location: indicates that the variance is relatively constant with the mean, but there are some potential outliers
\newline 4) residuals vs leverage: Cook's distance show that there are a few influential points, like China and India

```{r}
nrow(UN3_1)
model_0=lm(ModernC~.,data=UN3_1)
summary(model_0)
par(mfrow=c(2,2))
plot(model_0,ask=F)
```

5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?  
\newline -Answer: influential localities are China and India, Pop needs to be transformed
\newline -Explanation: In the added variable plots, the one for ModernC and Pop (2nd row, on the right side) caught my attention because the data is clustered together. There are two influential observations (China and India) are far away from the other observations, indicating that the variable Pop needs to be transformed before we use it as a prediction for ModernC.

```{r}

par(mfrow=c(3,2))
avPlots(model_0)

```

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.
\vspace{4mm}
\newline - Answer: The range of Change is [-1.10  3.62], which include negative values. So I add 2.1 to all the values for Change to make the range become [1, 5.72]. There are 2 reasons: 1) I did not use abs() function, because the Change indicates the percentage of Annual population growth rate, we may interested in looking at the difference of Change for different counties, so we don’t want to use abs(), 2) I want the minimum value of Change to be 1, so if there is a need for log transformation, all the values would be positive after the log transformation.
\newline - Based on the graphical EDA, I think the following variables need transformation: PPdgp and Pop
\newline - Reasons for transformations: 1) PPdgp has MLE of Lambda of -0.129, which is very close to 0, so I will do log transformation, 2) Pop as MLE of Lambda of 0.407, which is very close to 0.5, so I will do the square root transformation.
\newline - So at this point, my model 1 will be ModernC~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban
```{r}
# 1 Change
range(UN3_1$Change)
UN3_2=UN3_1 #create a new dataframe
UN3_2$Change=UN3_2$Change+2.1
range(UN3_2$Change)

# transform two variables
car::boxTidwell(ModernC~Pop+PPgdp,~Change+Frate+Fertility+Purban,data=UN3_1)
```

7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.
\vspace{4mm}
\newline Answer: with the boxCox results, we can see that the 95% interval for lambda is about 0.6 to 1. So I think it is not necessary to transform y at this point because $\lambda=1$ indicates no transformation is needed.
```{r}
# regression after tranformation for x
model_1=lm(ModernC~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban,data=UN3_2)

MASS::boxcox(model_1)
```

8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.
\vspace{4mm}
\newline -Answer: From the regression results of Model 1, we can see that compared to the original model in Ex.4, Model 1 is better thanks to the transformation. Because the residual plots show better results, and R-squared values are improved.
\newline -Although the boxCox plot indicates that y does not need to be transformed, I am curious to see if transforming y can result in better regression results. So I tried two new models with transformed y: model 2 (use log transformation for y) and model 3 (use square root transformation for y)
\vspace{2mm}
\newline -Explanations: 1) log transformation is a common way to transform variable. Since I already have log transformation for one of my predictors PPgdp, I want to see if log transformation works for y too. 2) values of y might have a poisson distribution, and the squart root transformation is very common for poisson variables, so I want to try it.
\vspace{2mm}
\newline -Results: with the regression results of model 2 and model 3, we can see that Model 3 fits slightly better than Model 2. Both models have slightly better R-squared values than model 1. But there is no obvious improvements in terms of graphical EDA and after y transformation, a few predictors (for example, Frate and Pop) are no longer significant in terms of their p-values. So I think Model 1 is still the best model I’ve got at this point.

```{r}
summary(model_1)
par(mfrow=c(2,2))
plot(model_1,ask=F)
par(mfrow=c(2,2))
avPlots(model_1)

# try log transformation for y
model_2=lm(log(ModernC)~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban,data=UN3_2)
summary(model_2)
par(mfrow=c(2,2))
plot(model_2,ask=F)
par(mfrow=c(3,2))
avPlots(model_2)

# try sqrt transformation for y
model_3=lm(sqrt(ModernC)~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban,data=UN3_2)
summary(model_3)
par(mfrow=c(2,2))
plot(model_3,ask=F)
par(mfrow=c(3,2))
avPlots(model_3)

```

9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?
\vspace{4mm}
\newline Answer: No I did not get a different model. 
\newline Explanations: Step 1: to first find the best transformation of the response, I used Model 0 (original full model) for boxCox. And the resulted plot is similar to the one in Ex. 7, which indicates that y does not need to be transformed. 
\newline Step 2: use the Box-Tidwell to find appropriate transformations of the predictor variables. The MLE of lambda are the same as that in Ex. 6. So Model 1 is still the best model so far.

```{r }
# use Model 0 for boxCox
MASS::boxcox(model_0)

# transform two variables
car::boxTidwell(ModernC~Pop+PPgdp,~Change+Frate+Fertility+Purban,data=UN3_2)

```

10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.
\vspace{4mm}
\newline Answer: Yes I think China and India are two influential points although I did transformation to Pop. So I removed them to check the results of Model 1. The regression results show that after removing the influential points, R-squared values does not change much and sqrt(Pop) becomes a less significant predictor in terms of p-value. But the residual plots and the added variable plots show a better results, especially for Pop. So I think we should remove the points in the regression.
```{r}
# remove outliers
ouliers=c("China","India")
UN3_3=UN3_2[which(!(rownames(UN3_2) %in% ouliers)),]
model_1rm=lm(ModernC~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban,data=UN3_3)
summary(model_1rm)
par(mfrow=c(2,2))
plot(model_1rm,ask=F)

par(mfrow=c(2,2))
avPlots(model_1rm)
#check termplot
termplot(model_1,term="sqrt(Pop)",xlabs="Pop with outliers",partial.resid=T, se=T, rug=T,smooth=panel.smooth)
termplot(model_1rm,term="sqrt(Pop)",xlabs="Pop after outliers removed",partial.resid=T, se=T, rug=T,smooth=panel.smooth)
```

## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units! 
\vspace{4mm}
\newline Answer: ModernC positively correlates to all predictors except Fertility and Purban. 
\newline 1) For predictors Change and Frate, when keep other predictors unchanged, one unit increase in the predictors would result in 4.838 and 0.182 unit increase in the response variable ModernC, respectively.
\newline 2) For predictors Fertility and Purban, when keep other predictors unchanged, one unit increase in the predictors would result in 9.319 and 0.029 unit decrease in the response variable ModernC, respectively.
\newline 3) For PPgdp, when keep other predictors unchanged, 10% increase in the predictor would result in $5.183*log(1.1)$ unit increase in the response variable ModernC
\newline 4) For Pop, when keep other predictors unchanged, 10% unit increase in the predictor would result in $0.024*sqrt(1.1)$ unit increase in the response variable ModernC

```{r}

re_table=cbind(round(confint(model_1rm),3),round(coef(model_1rm),3))
re_table=as.data.frame(re_table)
names(re_table)=c("CI 2.5%","CI 97.5%","Estimate")
kable(re_table,caption=c("95% confidence intervals and estimates for final model"))
```


12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model
\vspace{4mm}
\newline Answer: After removing two influential points (China and India), my best model is model 1: ModernC~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban
\newline I chose the best model based on the following criteria:
\newline 1) Graphical EDA. I picked the model has relatively the best result from residual plots and added variable plots. That’s why removing outliers are necessary because they improved the residual results.
\newline 2) Model complexity. I want an accurate model but I don’t want over-fitting. And the model should be easy to explain and understand. That’s why although transforming y may result in slightly better accuracy, I did not use y transformation because it would be harder to explain the model, and it’s more complex, too.
\newline 3) Number of significant predictors, Since I am not dropping any predictors, I hope all the variables play a role in the model. So I prefer models in which most predictors are significant.


## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept._
$$e_y=\hat{\beta_0}+\hat{\beta_1}e_{x_i}$$
$$(I-H)Y=\hat{\beta_0}+\hat{\beta_1}(I-H)x_i$$
$\text{since OLS for}\beta$ is $(X'X)^{-1}X'Y$, here X equals to $(I-H)x_i$,and Y equals to $(I-H)Y$, so
$$(I-H)Y=1_n'\hat{\beta_0}+[((I-H)x_i)'((I-H)x_i)]'((I-H)x_i)'(I-H)Y(I-H)x_i$$
$$=1_n'\hat{\beta_0}+[x_i'(I-H)'(I-H)x_i]'x_i'(I-H)'(I-H)Y(I-H)x_i$$
since $(I-H)'=(I-H)$, and $(I-H)(I-H)=(I-H)$
$$(I-H)Y=1_n'\hat{\beta_0}+[x_i'(I-H)x_i]'x_i'(I-H)Y(I-H)x_i$$
since the terms $[x_i'(I-H)x_i]'$ and $x_i'(I-H)Y$ are both scalars, we can move them around in the equation. And we can multiply $x_i'$ to both sides of the equation and get:
$$x_i'(I-H)Y=x_i'\hat{\beta_0}+x_i'(I-H)x_i[x_i'(I-H)x_i]'x_i'(I-H)Y$$
$$x_i'(I-H)Y=x_i'\hat{\beta_0}+x_i'(I-H)Y$$
$$0=x_i'\hat{\beta_0}$$
The above equation is valid for a random $x_i$ so $\hat{\beta_0}=0$

14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 
\vspace{4mm}
\newline Answer: Regress "ModernC" on all predictors except "Change" and obtain the residuals; call them $e_Y$. Regress "Change" on the other predictors and obtain the residuals (call $e_{X1}$). Then I regress $e_Y$ on $e_X$. The results show that the slope for $e_Y$ on $e_X$ is the same as the estimated slope for "Change" in the full model with all the predictors.
```{r }
e_Y = residuals(lm(ModernC~log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban, data=UN3_3))
e_X1 = residuals(lm(Change ~ log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban, data=UN3_3))
df = data.frame(e_Y = e_Y, e_X1 = e_X1)
ggplot(data=df, aes(x = e_X1, y = e_Y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

summary(model_1rm)$coef
summary(lm(e_Y ~ e_X1, data=df))$coef
```






